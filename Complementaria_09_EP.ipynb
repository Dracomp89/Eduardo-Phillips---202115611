{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dracomp89/Eduardo-Phillips---202115611/blob/main/Complementaria_09_EP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b713ac29-2d3a-4932-9d8b-6b222d37ab2e",
      "metadata": {
        "id": "b713ac29-2d3a-4932-9d8b-6b222d37ab2e"
      },
      "source": [
        "> Complementaria métodos computacionales 1\n",
        ">\n",
        "> Semana 09\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1ba408d-26dd-4a3a-a431-7d776407bd3a",
      "metadata": {
        "id": "b1ba408d-26dd-4a3a-a431-7d776407bd3a"
      },
      "source": [
        "# 1) Descenso gradiente (4pt)\n",
        "\n",
        "Dado un set de datos (dos arrays 1-d) `x` y `y`, y una función definida como `f(x,alpha)`, implemente una función que encuentre, mediante descenso gradiente, el vector de parámetros `alpha` que minimice la suma de cuadrados de los errores de `y` con la función de modelo `f(x,alpha)`. Es decir,\n",
        "\n",
        "$$\n",
        "\\min_{\\vec{\\alpha}} \\sum_{i=1}^N \\left(y_i - f(x_i,\\vec{\\alpha})\\right)^2\n",
        "$$\n",
        "\n",
        "La función debe tener la siguiente signatura:\n",
        "\n",
        "```python\n",
        "def gradient_descent_least_squares(model_function, x_data, y_data, initial_parameters, learning_rate):\n",
        "```\n",
        "\n",
        "\n",
        "Para probar su algoritmo, invéntese alguna función no lineal (por ejemplo $\\alpha_1 e^{-\\alpha_1 x} \\sin(\\alpha_2 x)$, o algo así), genere datos de prueba, y agrégueles ruido, e intente encontrar los parámetros con los cuales generó los datos.\n",
        "\n",
        "También hay un ejemplo de datos en bloque neón. Intente encontrar la función a la cual corresponden esos datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3f34fb68-68df-405b-bfc5-f8f7380db083",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f34fb68-68df-405b-bfc5-f8f7380db083",
        "outputId": "872814a6-7b63-4ac9-8a92-e3bfe92bb2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parámetros optimizados: [-5.53645013  2.07226604 -7.45472829]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definir la función no lineal para el ajuste\n",
        "def model_function(x, alpha):\n",
        "    return alpha[0] * np.exp(-alpha[1] * x) * np.sin(alpha[2] * x)\n",
        "\n",
        "# Función de descenso por gradiente\n",
        "def gradient_descent_least_squares(model_function, x_data, y_data, initial_parameters, learning_rate, max_iterations=1000, tolerance=1e-6):\n",
        "    alpha = np.array(initial_parameters, dtype=np.float64)\n",
        "    N = len(y_data)\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        # Calcular los valores predichos por el modelo\n",
        "        predictions = model_function(x_data, alpha)\n",
        "\n",
        "        # Calcular el error entre las predicciones y los datos reales\n",
        "        error = predictions - y_data\n",
        "\n",
        "        # Calcular el gradiente\n",
        "        grad = np.zeros_like(alpha)\n",
        "        for i in range(len(alpha)):\n",
        "            # Derivada numérica del modelo con respecto a cada parámetro\n",
        "            alpha_step = np.copy(alpha)\n",
        "            alpha_step[i] += 1e-5  # Pequeño incremento para la derivada numérica\n",
        "            grad[i] = (np.sum((model_function(x_data, alpha_step) - y_data)**2) - np.sum((predictions - y_data)**2)) / 1e-5\n",
        "\n",
        "        # Actualizar los parámetros usando el gradiente y la tasa de aprendizaje\n",
        "        alpha -= learning_rate * grad\n",
        "\n",
        "        # Calcular el error cuadrático medio\n",
        "        mse = np.mean(error**2)\n",
        "        if mse < tolerance:\n",
        "            print(f\"Convergió en {iteration} iteraciones.\")\n",
        "            break\n",
        "\n",
        "    return alpha\n",
        "\n",
        "# Cargar los datos desde el archivo usando np.loadtxt\n",
        "file_path = '/content/datos_ejemplo_ajuste.dat'  # Cambia a la ruta correcta\n",
        "data = np.loadtxt(file_path)\n",
        "\n",
        "# Separar los datos en x e y\n",
        "x_data, y_data = data[:, 0], data[:, 1]\n",
        "\n",
        "# Parámetros iniciales para el descenso por gradiente\n",
        "initial_parameters = [1, 1, 1]\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Llamar a la función de descenso por gradiente\n",
        "alpha_optimized = gradient_descent_least_squares(model_function, x_data, y_data, initial_parameters, learning_rate)\n",
        "\n",
        "print(\"Parámetros optimizados:\", alpha_optimized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ce63619-c01f-4b22-abc9-0ebcc89fbefe",
      "metadata": {
        "id": "8ce63619-c01f-4b22-abc9-0ebcc89fbefe"
      },
      "source": [
        "# 2) Comodidades\n",
        "\n",
        "\n",
        "## 2.a) Vectorización (1pt)\n",
        "\n",
        "Haga que el parámetro de condiciones iniciales esté vectorizado, es decir, que cuando sea una lista de parámetros, retorne una lista de resultados del método empezando desde cada especificación de parámetros iniciales.\n",
        "\n",
        "No recomiendo usar `np.vectorize`, pues aunque es posible, es complicado usarlo en este caso.\n",
        "\n",
        "## 2.b) Descenso estocástico (1pt)\n",
        "\n",
        "agregue un parámetro a la función `train_test_divide=False`, que cuando sea un número diferente de cero, realice cada paso del descenso gradiente con una selección aleatoria de los datos `x` y `y`.\n",
        "\n",
        "Por ejemplo, si `train_test_divide=0.35`, cada paso se debe realizar seleccionando al azar el 35% de las parejas (x,y)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "254a9bce-ae7e-4a5d-8071-ff8b245df4d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "254a9bce-ae7e-4a5d-8071-ff8b245df4d1",
        "outputId": "9cb69b25-b69b-47e0-c538-04f0583ed5c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parámetros optimizados: [array([3.27645462, 0.97519928, 0.37335323]), array([5.53086858, 2.04372203, 7.4666428 ]), array([5.53585564, 2.10838071, 7.42451473])]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definir la función no lineal para el ajuste\n",
        "def model_function(x, alpha):\n",
        "    return alpha[0] * np.exp(-alpha[1] * x) * np.sin(alpha[2] * x)\n",
        "\n",
        "# Función de descenso por gradiente\n",
        "def gradient_descent_least_squares(model_function, x_data, y_data, initial_parameters, learning_rate, max_iterations=1000, tolerance=1e-6, train_test_divide=False):\n",
        "    if isinstance(initial_parameters[0], list) or isinstance(initial_parameters[0], np.ndarray):\n",
        "        # Si initial_parameters es una lista de listas, llamar recursivamente para cada conjunto de parámetros\n",
        "        results = [gradient_descent_least_squares(model_function, x_data, y_data, params, learning_rate, max_iterations, tolerance, train_test_divide) for params in initial_parameters]\n",
        "        return results\n",
        "\n",
        "    alpha = np.array(initial_parameters, dtype=np.float64)\n",
        "    N = len(y_data)\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        if train_test_divide:\n",
        "            # Si train_test_divide es diferente de cero, seleccionar un porcentaje de los datos aleatoriamente\n",
        "            indices = np.random.choice(N, int(N * train_test_divide), replace=False)\n",
        "            x_batch = x_data[indices]\n",
        "            y_batch = y_data[indices]\n",
        "        else:\n",
        "            x_batch = x_data\n",
        "            y_batch = y_data\n",
        "\n",
        "        # Calcular los valores predichos por el modelo\n",
        "        predictions = model_function(x_batch, alpha)\n",
        "\n",
        "        # Calcular el error entre las predicciones y los datos reales\n",
        "        error = predictions - y_batch\n",
        "\n",
        "        # Calcular el gradiente\n",
        "        grad = np.zeros_like(alpha)\n",
        "        for i in range(len(alpha)):\n",
        "            # Derivada numérica del modelo con respecto a cada parámetro\n",
        "            alpha_step = np.copy(alpha)\n",
        "            alpha_step[i] += 1e-5  # Pequeño incremento para la derivada numérica\n",
        "            grad[i] = (np.sum((model_function(x_batch, alpha_step) - y_batch)**2) - np.sum((predictions - y_batch)**2)) / 1e-5\n",
        "\n",
        "        # Actualizar los parámetros usando el gradiente y la tasa de aprendizaje\n",
        "        alpha -= learning_rate * grad\n",
        "\n",
        "        # Calcular el error cuadrático medio\n",
        "        mse = np.mean(error**2)\n",
        "        if mse < tolerance:\n",
        "            print(f\"Convergió en {iteration} iteraciones.\")\n",
        "            break\n",
        "\n",
        "    return alpha\n",
        "\n",
        "# Cargar los datos desde el archivo usando np.loadtxt\n",
        "file_path = '/content/datos_ejemplo_ajuste.dat'  # Cambia a la ruta correcta\n",
        "data = np.loadtxt(file_path)\n",
        "\n",
        "# Separar los datos en x e y\n",
        "x_data, y_data = data[:, 0], data[:, 1]\n",
        "\n",
        "# Parámetros iniciales para el descenso por gradiente (vectorizado)\n",
        "initial_parameters = [[1, 1, 1], [0.5, 1.5, 0.8], [1.2, 0.8, 1.1]]\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Llamar a la función de descenso por gradiente con descenso estocástico (ejemplo con 35% de datos seleccionados aleatoriamente)\n",
        "train_test_divide = 0.35\n",
        "\n",
        "alpha_optimized = gradient_descent_least_squares(model_function, x_data, y_data, initial_parameters, learning_rate, train_test_divide=train_test_divide)\n",
        "\n",
        "print(\"Parámetros optimizados:\", alpha_optimized)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dba01f7-a954-4ed2-aef4-4ee6d6f118bd",
      "metadata": {
        "id": "9dba01f7-a954-4ed2-aef4-4ee6d6f118bd"
      },
      "source": [
        "# 3) bonos\n",
        "\n",
        "Encuentre una función que describa los datos `datos_09.npz`.\n",
        "\n",
        "Si usa un polinomio o una serie de Fourier, encuentre el orden \"óptimo\", investigue qué significa eso.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bfe0df2-7d1a-4ab1-bf71-48a1a1223a6d",
      "metadata": {
        "id": "6bfe0df2-7d1a-4ab1-bf71-48a1a1223a6d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}